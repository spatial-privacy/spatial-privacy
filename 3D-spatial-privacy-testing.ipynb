{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import scipy.io\n",
    "import time\n",
    "import h5py\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n",
    "from mpl_toolkits.axes_grid1.axes_divider import make_axes_locatable\n",
    "\n",
    "from numpy import linalg as LA\n",
    "from scipy.spatial import Delaunay\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "#sys.path.insert(0, \"../\")\n",
    "from info3d import *\n",
    "from nn_matchers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXTRACTING the existing sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('point_collection/new_contiguous_point_collection.pickle','rb') as f: \n",
    "    new_contiguous_point_collection = pickle.load(f)\n",
    "    \n",
    "with open('descriptors/new_complete_res5_4by5_descriptors.pickle','rb') as f:\n",
    "    descriptors = pickle.load(f)\n",
    "\n",
    "\"\"\"\n",
    "Parameters\n",
    "\"\"\"\n",
    "# We used a radius range of 0.25 to 5.0 in increments of 0.25\n",
    "radius_range = np.arange(0.5,1.6,0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1.1: Testing with Partial Spaces "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Testing our partial samples: Raw spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for radius in radius_range:\n",
    "        \n",
    "    t0 = time.time()\n",
    "    \n",
    "    with open('testing_samples/{}_partial_point_cloud.pickle'.format(radius), 'rb') as f:\n",
    "        partial_point_collection = pickle.load(f)\n",
    "        samples = len(partial_point_collection)\n",
    "        \n",
    "    partial_scores = []\n",
    "    \n",
    "    for obj_meta, partial_pointcloud, partial_triangles in partial_point_collection:\n",
    "        \n",
    "        # ROTATION\n",
    "        random_theta =  (2*np.pi)*np.random.random()# from [0, 2pi)\n",
    "        random_axis = np.random.choice(np.arange(0,3))\n",
    "        rotated_pointCloud = rotatePointCloud(partial_pointcloud, random_theta, random_axis)\n",
    "\n",
    "        # TRANSLATION\n",
    "        t_pointCloud = np.asarray(rotated_pointCloud)\n",
    "        random_tx_axis = np.random.choice(np.arange(0,3))\n",
    "        random_translation = np.random.random()\n",
    "        t_pointCloud[:,random_tx_axis] = t_pointCloud[:,random_tx_axis] + random_translation\n",
    "        t_triangles = np.asarray(partial_triangles)#-vertices_length\n",
    "\n",
    "        t1 = time.time()\n",
    "\n",
    "        try:\n",
    "            p_descriptors, p_keypoints, p_d_c = getSpinImageDescriptors(\n",
    "                t_pointCloud,\n",
    "                down_resolution = 5,\n",
    "                cylindrical_quantization = [4,5]\n",
    "            )\n",
    "        except Exception as ex:\n",
    "            print(\"Error getting descriptors of\",obj_meta)\n",
    "            print(\"Error Message:\",ex)\n",
    "            \n",
    "            continue\n",
    "\n",
    "        # Resetting the diff_Ratio matrix\n",
    "        diff_scores = np.ones((p_descriptors.shape[0],len(descriptors),2))\n",
    "        diff_ratios = np.ones((p_descriptors.shape[0],len(descriptors)))\n",
    "        diff_indexs = np.ones((p_descriptors.shape[0],len(descriptors),2))\n",
    "\n",
    "        #print(diff_ratios.shape)\n",
    "        local_keypoint_matches = []\n",
    "\n",
    "        for i_r, ref_descriptor in enumerate(descriptors):\n",
    "\n",
    "            r_descriptors = ref_descriptor[1]\n",
    "            r_keypoints = ref_descriptor[2]\n",
    "\n",
    "            matching_range = np.arange(r_descriptors.shape[1])\n",
    "\n",
    "            try:    \n",
    "                f_nearestneighbor, diff = getMatches(p_descriptors,r_descriptors,2,range_to_match=matching_range)\n",
    "                diff = diff/np.amax(diff) # max-normalization of differences\n",
    "                diff_ratio = diff[:,0]/diff[:,1]\n",
    "                diff_ratios[:,i_r] = diff_ratio\n",
    "                diff_scores[:,i_r] = diff\n",
    "                diff_indexs[:,i_r] = f_nearestneighbor\n",
    "                \n",
    "                # Taking note of the matched keypoints\n",
    "                local_keypoint_matches.append([\n",
    "                    obj_meta,\n",
    "                    p_keypoints,\n",
    "                    r_keypoints[f_nearestneighbor[:,0]]\n",
    "                ])\n",
    "\n",
    "            except Exception as ex:\n",
    "                print(rotation,\"Error Matching:\",ex)\n",
    "\n",
    "        # Accumulating the diff_ratio matrix for every partial (rotated) object\n",
    "        partial_scores.append([\n",
    "            obj_meta,\n",
    "            np.asarray(diff_ratios),\n",
    "            np.asarray(diff_indexs),\n",
    "            np.asarray(diff_scores),\n",
    "            local_keypoint_matches\n",
    "        ])\n",
    "\n",
    "        if len(partial_scores) % 10 == (samples-1)%10:\n",
    "            #print('Test')\n",
    "            print(\" radius = {}: Done with {} iterations. Time to match {:.3f} seconds.\".format(\n",
    "                radius,\n",
    "                len(partial_scores),\n",
    "                time.time()-t0)\n",
    "                 )\n",
    "            t0 = time.time()\n",
    "        \n",
    "            current_errors = NN_matcher(partial_scores)\n",
    "            print(\"   Error Rate:\",np.sum(current_errors[:,1]/len(partial_scores)))\n",
    "\n",
    "    partial_errors = NN_matcher(partial_scores)\n",
    "    print(radius,\"Error Rate:\",np.sum(partial_errors[:,1]/len(partial_scores)))\n",
    "                                                                                \n",
    "    with open('testing_results/partials/radius_{}_RAW_scores.pickle'.format(radius), 'wb') as f:\n",
    "        pickle.dump(partial_scores,f)\n",
    "                                              \n",
    "    with open('testing_results/partials/radius_{}_RAW_errors.pickle'.format(radius), 'wb') as f:\n",
    "        pickle.dump(partial_errors,f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Testing our partial samples: RANSAC spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for radius in radius_range:\n",
    "        \n",
    "    t0 = time.time()\n",
    "    \n",
    "    with open('testing_samples/{}_partial_point_cloud.pickle'.format(radius), 'rb') as f:\n",
    "        partial_point_collection = pickle.load(f)\n",
    "        samples = len(partial_point_collection)\n",
    "        \n",
    "    partial_scores = []\n",
    "    partial_properties = []\n",
    "    \n",
    "    for obj_meta, partial_pointcloud, partial_triangles in partial_point_collection:\n",
    "        \n",
    "        # ROTATION\n",
    "        random_theta =  (2*np.pi)*np.random.random()# from [0, 2pi)\n",
    "        random_axis = np.random.choice(np.arange(0,3))\n",
    "        rotated_pointCloud = rotatePointCloud(partial_pointcloud, random_theta, random_axis)\n",
    "\n",
    "        # TRANSLATION\n",
    "        t_pointCloud = np.asarray(rotated_pointCloud)\n",
    "        random_tx_axis = np.random.choice(np.arange(0,3))\n",
    "        random_translation = np.random.random()\n",
    "        t_pointCloud[:,random_tx_axis] = t_pointCloud[:,random_tx_axis] + random_translation\n",
    "        t_triangles = np.asarray(partial_triangles)#-vertices_length\n",
    "\n",
    "        #if object_name % 13 == 0:\n",
    "        #    print('{}: Processing iteration {} of object {}.'.format(radius,iteration,object_name))\n",
    "\n",
    "        t1 = time.time()\n",
    "\n",
    "        # GETTING GENERALIZATION\n",
    "        gen_planes = getRansacPlanes(\n",
    "            t_pointCloud\n",
    "        )\n",
    "\n",
    "        p_pointcloud, p_triangles = getGeneralizedPointCloud(\n",
    "            planes=gen_planes, \n",
    "        )\n",
    "\n",
    "        try:\n",
    "            p_descriptors, p_keypoints, p_d_c = getSpinImageDescriptors(\n",
    "                p_pointcloud,\n",
    "                down_resolution = 5,\n",
    "                cylindrical_quantization = [4,5]\n",
    "            )\n",
    "        except Exception as ex:\n",
    "            print(\"Error getting descriptors of\",obj_meta)\n",
    "            print(\"Error Message:\",ex)\n",
    "            \n",
    "            continue\n",
    "\n",
    "        # Resetting the diff_Ratio matrix\n",
    "        diff_scores = np.ones((p_descriptors.shape[0],len(descriptors),2))\n",
    "        diff_ratios = np.ones((p_descriptors.shape[0],len(descriptors)))\n",
    "        diff_indexs = np.ones((p_descriptors.shape[0],len(descriptors),2))\n",
    "\n",
    "        #print(diff_ratios.shape)\n",
    "        local_keypoint_matches = []\n",
    "\n",
    "        for i_r, ref_descriptor in enumerate(descriptors):\n",
    "\n",
    "            #o_, r_descriptors, r_keypoints, r_d_c\n",
    "            r_descriptors = ref_descriptor[1]\n",
    "            r_keypoints = ref_descriptor[2]\n",
    "\n",
    "            matching_range = np.arange(r_descriptors.shape[1])\n",
    "\n",
    "            try:    \n",
    "                f_nearestneighbor, diff = getMatches(p_descriptors,r_descriptors,2,range_to_match=matching_range)\n",
    "                diff = diff/np.amax(diff) # max-normalization of differences\n",
    "                diff_ratio = diff[:,0]/diff[:,1]\n",
    "                diff_ratios[:,i_r] = diff_ratio\n",
    "                diff_scores[:,i_r] = diff\n",
    "                diff_indexs[:,i_r] = f_nearestneighbor\n",
    "                \n",
    "                # Taking note of the matched keypoints\n",
    "                local_keypoint_matches.append([\n",
    "                    obj_meta,\n",
    "                    p_keypoints,\n",
    "                    r_keypoints[f_nearestneighbor[:,0]]\n",
    "                ])\n",
    "\n",
    "            except Exception as ex:\n",
    "                print(rotation,\"Error Matching:\",ex)\n",
    "\n",
    "        # Accumulating the diff_ratio matrix for every partial (rotated) object\n",
    "        partial_scores.append([\n",
    "            obj_meta,\n",
    "            np.asarray(diff_ratios),\n",
    "            np.asarray(diff_indexs),\n",
    "            np.asarray(diff_scores),\n",
    "            local_keypoint_matches\n",
    "        ])\n",
    "\n",
    "        if len(partial_scores) % 10 == (samples-1)%10:\n",
    "            #print('Test')\n",
    "            print(\" radius = {}: Done with {} iterations. Time to match {:.3f} seconds.\".format(\n",
    "                radius,\n",
    "                len(partial_scores),\n",
    "                time.time()-t0)\n",
    "                 )\n",
    "            t0 = time.time()\n",
    "        \n",
    "            current_errors = NN_matcher(partial_scores)\n",
    "            print(\"   Error Rate:\",np.sum(current_errors[:,1]/len(partial_scores)))\n",
    "\n",
    "    partial_errors = NN_matcher(partial_scores)\n",
    "    print(radius,\"Error Rate:\",np.sum(partial_errors[:,1]/len(partial_scores)))\n",
    "                                                                                \n",
    "    with open('testing_results/partials/radius_{}_RANSAC_scores.pickle'.format(radius), 'wb') as f:\n",
    "        pickle.dump(partial_scores,f)\n",
    "                                              \n",
    "    with open('testing_results/partials/radius_{}_RANSAC_errors.pickle'.format(radius), 'wb') as f:\n",
    "        pickle.dump(partial_errors,f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 Results of partial spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(9, 3))\n",
    "\n",
    "RawNN = []\n",
    "RansacGeneralizedNN = []\n",
    "\n",
    "RawNN_intra_errors = []\n",
    "RansacGeneralizedNN_intra_errors = []\n",
    "\n",
    "for radius in radius_range:\n",
    "    \n",
    "    try:\n",
    "        with open('testing_results/partials/radius_{}_RAW_errors.pickle'.format(radius), 'rb') as f:\n",
    "            partial_errors = pickle.load(f)\n",
    "\n",
    "        RawNN.append([\n",
    "            radius,\n",
    "            np.mean(partial_errors[:,1]),\n",
    "            np.std(partial_errors[:,1]),\n",
    "        ])\n",
    "        \n",
    "        correct_interspace_labels_idxs = np.where(partial_errors[:,1]==0)[0]\n",
    "\n",
    "        intraspace_errors  = partial_errors[correct_interspace_labels_idxs,2]\n",
    "\n",
    "        RawNN_intra_errors.append([\n",
    "            radius,\n",
    "            np.nanmean(intraspace_errors),\n",
    "            np.nanstd(intraspace_errors)\n",
    "        ])\n",
    "        \n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        with open('testing_results/partials/radius_{}_RANSAC_errors.pickle'.format(radius), 'rb') as f:\n",
    "            partial_errors = pickle.load(f)\n",
    "\n",
    "        RansacGeneralizedNN.append([\n",
    "            radius,\n",
    "            np.nanmean(partial_errors[:,1]),\n",
    "            np.nanstd(partial_errors[:,1]),\n",
    "        ])\n",
    "\n",
    "        correct_interspace_labels_idxs = np.where(partial_errors[:,1]==0)[0]\n",
    "\n",
    "        intraspace_errors  = partial_errors[correct_interspace_labels_idxs,2]\n",
    "\n",
    "        RansacGeneralizedNN_intra_errors.append([\n",
    "            radius,\n",
    "            np.nanmean(intraspace_errors),\n",
    "            np.nanstd(intraspace_errors)\n",
    "        ])\n",
    "        \n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "RansacGeneralizedNN = np.asarray(RansacGeneralizedNN)\n",
    "RawNN = np.asarray(RawNN)\n",
    "\n",
    "RawNN_intra_errors = np.asarray(RawNN_intra_errors)\n",
    "RansacGeneralizedNN_intra_errors = np.asarray(RansacGeneralizedNN_intra_errors)\n",
    "\n",
    "ax1 = fig.add_subplot(121) \n",
    "\n",
    "ax1.grid(alpha = 0.7)\n",
    "ax1.set_ylim(-0.025,1.025)\n",
    "ax1.set_xlim(radius_range[0]-0.25,radius_range[-1]+0.25)\n",
    "markersize = 8\n",
    "\n",
    "ax1.set_ylabel(\"INTER-space Privacy\")\n",
    "ax1.set_xlabel(\"Partial Radius\")\n",
    "#ax1.set_yticklabels(fontsize = 16)\n",
    "#ax1.set_xticklabels(fontsize = 16)\n",
    "\n",
    "ax1.plot(\n",
    "    RawNN[:,0],RawNN[:,1],\n",
    "    \"-o\",\n",
    "    linewidth = 2,\n",
    "    mew = 2,markersize = markersize,\n",
    "    label = \"Raw\"\n",
    ")\n",
    "ax1.plot(\n",
    "    RansacGeneralizedNN[:,0],RansacGeneralizedNN[:,1],\n",
    "    \"-s\",\n",
    "    linewidth = 2,\n",
    "    mew = 2,markersize = markersize,\n",
    "    label = \"RANSAC\"\n",
    ")\n",
    "\n",
    "ax1.legend(loc = \"lower left\")\n",
    "\n",
    "ax2 = fig.add_subplot(122) \n",
    "\n",
    "ax2.grid(alpha = 0.7)\n",
    "ax2.set_ylim(-0.25,10.25)\n",
    "ax2.set_xlim(radius_range[0]-0.25,radius_range[-1]+0.25)\n",
    "\n",
    "ax2.set_ylabel(\"INTRA-space Privacy\")\n",
    "ax2.set_xlabel(\"Partial Radius\")\n",
    "#ax2.set_yticklabels(fontsize = 16)\n",
    "#ax2.set_xticklabels(fontsize = 16)\n",
    "\n",
    "plt.minorticks_on()\n",
    "\n",
    "ax2.plot(\n",
    "    RawNN_intra_errors[:,0],\n",
    "    RawNN_intra_errors[:,1], \n",
    "    linewidth = 2,\n",
    "    marker = 'o',fillstyle = 'none',\n",
    "    mew = 2,markersize = markersize,\n",
    "    label = \"Raw\"\n",
    ")\n",
    "\n",
    "ax2.plot(\n",
    "    RansacGeneralizedNN_intra_errors[:,0],\n",
    "    RansacGeneralizedNN_intra_errors[:,1], \n",
    "    linewidth = 2, \n",
    "    marker = 's',fillstyle = 'none',\n",
    "    mew = 2,markersize = markersize,\n",
    "    label = \"RANSAC\"\n",
    ")\n",
    "\n",
    "ax2.legend(loc = \"lower left\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 Testing with successively released partial spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Parameters\n",
    "\"\"\"\n",
    "# We used a radius range of 0.25 to 5.0 in increments of 0.25.\n",
    "radius_range = radius_range[:2]\n",
    "\n",
    "# For our work, we orignally used 50 samples with further 100 successive releases for our investigation.\n",
    "# Below are lower parameters, change as desired.\n",
    "#samples = 50\n",
    "#releases = 50\n",
    "\n",
    "# For demonstration purposes, we skip testing some successive samples but we still accumulate them.\n",
    "skip = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Testing the successive case: RAW and RANSAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for radius in radius_range:\n",
    "        \n",
    "    t0 = time.time()\n",
    "    \n",
    "    try:\n",
    "        with open('testing_samples/{}_successive_point_cloud.pickle'.format(radius), 'rb') as f:\n",
    "            successive_point_collection = pickle.load(f)\n",
    "            samples = len(successive_point_collection)\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    successive_scores = []\n",
    "    successive_errors = []\n",
    "    \n",
    "    g_successive_scores = []\n",
    "    g_successive_errors = []\n",
    "\n",
    "    for obj_, growing_point_collection in successive_point_collection:\n",
    "        \n",
    "        iteration_scores = []\n",
    "        \n",
    "        g_iteration_scores = []\n",
    "        \n",
    "        # ROTATION param\n",
    "        random_theta =  (2*np.pi)*np.random.random()# from [0, 2pi)\n",
    "        random_axis = np.random.choice(np.arange(0,3))\n",
    "        \n",
    "        # TRANSLATION param\n",
    "        random_tx_axis = np.random.choice(np.arange(0,3))\n",
    "        random_translation = np.random.random()\n",
    "        \n",
    "        growing_point_cloud = []\n",
    "        growing_p_point_cloud = []\n",
    "        growing_p_triangles = []\n",
    "        \n",
    "        releases = len(growing_point_collection)\n",
    "        release_count = 0\n",
    "    \n",
    "        for obj_meta, partial_pointcloud, partial_triangles in growing_point_collection:\n",
    "            #i_obj, released_growing_point_collection in enumerate(growing_point_collection):\n",
    "\n",
    "            rotated_pointCloud = rotatePointCloud(partial_pointcloud, random_theta, random_axis)\n",
    "\n",
    "            # TRANSLATION\n",
    "            t_pointCloud = np.asarray(rotated_pointCloud)\n",
    "            t_pointCloud[:,random_tx_axis] = t_pointCloud[:,random_tx_axis] + random_translation\n",
    "            t_triangles = np.asarray(partial_triangles)#-vertices_length\n",
    "            \n",
    "            #Regular Accumulation\n",
    "            if len(growing_point_cloud) == 0:\n",
    "                growing_point_cloud = t_pointCloud\n",
    "\n",
    "            else:\n",
    "                growing_point_cloud = np.concatenate(\n",
    "                    (growing_point_cloud,t_pointCloud),\n",
    "                    axis=0\n",
    "                )\n",
    "                \n",
    "            #RANSAC generalizations\n",
    "            if len(growing_p_point_cloud) == 0:\n",
    "                gen_planes = getLOCALIZEDRansacPlanes(\n",
    "                    pointCloud = t_pointCloud,\n",
    "                    original_vertex = obj_meta[-1]\n",
    "                )\n",
    "            else:\n",
    "                gen_planes = updatePlanesWithSubsumption(\n",
    "                    new_pointCloud=t_pointCloud,\n",
    "                    existing_pointCloud=growing_p_point_cloud,\n",
    "                    planes_to_find = max(min(release_count,50),30),\n",
    "                    #verbose=True\n",
    "                )\n",
    "            \n",
    "            if len(gen_planes) == 0:\n",
    "                print(\"No gen planes after release\",release_count,growing_point_cloud.shape)\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                updated_point_cloud, updated_triangles = getGeneralizedPointCloud(\n",
    "                    planes = gen_planes,\n",
    "                    triangle_area_threshold = 0.2,#2.0*np.amax(getTriangleAreas(partial_pointCloud, partial_triangles))\n",
    "                    #verbose = True\n",
    "                )\n",
    "                growing_p_point_cloud = updated_point_cloud\n",
    "                growing_p_triangles = updated_triangles\n",
    "                \n",
    "                #print(\" Successful:\",release_count,len(growing_p_point_cloud), len(growing_p_triangles),partial_pointCloud.shape)\n",
    "            except Exception as ex:\n",
    "                print(\"Error getting updated point cloud in release\",release_count+1)\n",
    "                print(\" \",growing_p_point_cloud.shape, growing_p_triangles.shape,partial_pointCloud.shape)\n",
    "                #print(ex)\n",
    "                continue\n",
    "                \n",
    "            if len(growing_p_point_cloud) == 0:\n",
    "                continue\n",
    "                \n",
    "            release_count += 1\n",
    "            \n",
    "            if release_count % skip != 1: # skip \n",
    "                continue\n",
    "                \n",
    "            #Regular Processing\n",
    "            growing_point_cloud = np.unique(growing_point_cloud,axis=0)\n",
    "            \n",
    "            try:\n",
    "                p_descriptors, p_keypoints, p_d_c = getSpinImageDescriptors(\n",
    "                    growing_point_cloud,\n",
    "                    down_resolution = 5,\n",
    "                    cylindrical_quantization = [4,5]\n",
    "                )\n",
    "            except:\n",
    "                p_descriptors = []\n",
    "                p_keypoints = []\n",
    "\n",
    "            # Resetting the diff_Ratio matrix\n",
    "            diff_scores = np.ones((p_descriptors.shape[0],len(descriptors),2))\n",
    "            diff_ratios = np.ones((p_descriptors.shape[0],len(descriptors)))\n",
    "            diff_indexs = np.ones((p_descriptors.shape[0],len(descriptors),2))\n",
    "\n",
    "            local_keypoint_matches = []\n",
    "\n",
    "            for i_r, ref_descriptor in enumerate(descriptors):\n",
    "\n",
    "                r_descriptors = ref_descriptor[1]\n",
    "                r_keypoints = ref_descriptor[2]\n",
    "\n",
    "                matching_range = np.arange(r_descriptors.shape[1])\n",
    "\n",
    "                try:    \n",
    "                    f_nearestneighbor, diff = getMatches(p_descriptors,r_descriptors,2,range_to_match=matching_range)\n",
    "                    diff = diff/np.amax(diff) # max-normalization of differences\n",
    "                    diff_ratio = diff[:,0]/diff[:,1]\n",
    "                    diff_ratios[:,i_r] = diff_ratio\n",
    "                    diff_scores[:,i_r] = diff\n",
    "                    diff_indexs[:,i_r] = f_nearestneighbor\n",
    "\n",
    "                    # Taking note of the matched keypoints\n",
    "                    local_keypoint_matches.append([\n",
    "                        obj_meta,\n",
    "                        p_keypoints,\n",
    "                        r_keypoints[f_nearestneighbor[:,0]]\n",
    "                    ])\n",
    "\n",
    "                except Exception as ex:\n",
    "                    print(rotation,\"Error Matching:\",ex)\n",
    "\n",
    "            # Accumulating the diff_ratio matrix for every partial (rotated) object\n",
    "            iteration_scores.append([\n",
    "                obj_meta,\n",
    "                diff_ratios,\n",
    "                diff_indexs,\n",
    "                diff_scores,\n",
    "                local_keypoint_matches\n",
    "            ])\n",
    "\n",
    "            if release_count % 2 == 0:\n",
    "                #print('Test')\n",
    "                print(\"  radius = {}: Done with {} releases ({}) ({} samples). Time to match {:.3f} seconds. ({})\".format(\n",
    "                    radius,\n",
    "                    len(iteration_scores),\n",
    "                    release_count,\n",
    "                    len(successive_scores),\n",
    "                    time.time()-t0,\n",
    "                    growing_point_cloud.shape\n",
    "                )\n",
    "                     )\n",
    "                t0 = time.time()\n",
    "\n",
    "                current_errors = NN_matcher(iteration_scores)\n",
    "                print(\"   Error Rate:\",np.sum(current_errors[:,1]/len(iteration_scores)))\n",
    "            \n",
    "            #RANSAC Processing         \n",
    "            try:\n",
    "                p_descriptors, p_keypoints, p_d_c = getSpinImageDescriptors(\n",
    "                    growing_p_point_cloud,\n",
    "                    down_resolution = 5,\n",
    "                    cylindrical_quantization = [4,5]\n",
    "                )\n",
    "            except:\n",
    "                p_descriptors = []\n",
    "                p_keypoints = []\n",
    "                \n",
    "                print(\"Error getting descriptors at release\",release_count,\"; using next release as this release.\")\n",
    "                \n",
    "                #release_count -= 1\n",
    "\n",
    "                continue\n",
    "\n",
    "            # Resetting the diff_Ratio matrix\n",
    "            diff_scores = np.ones((p_descriptors.shape[0],len(descriptors),2))\n",
    "            diff_ratios = np.ones((p_descriptors.shape[0],len(descriptors)))\n",
    "            diff_indexs = np.ones((p_descriptors.shape[0],len(descriptors),2))\n",
    "\n",
    "            local_keypoint_matches = []\n",
    "\n",
    "            for i_r, ref_descriptor in enumerate(descriptors):\n",
    "\n",
    "                #o_, r_descriptors, r_keypoints, r_d_c\n",
    "                r_descriptors = ref_descriptor[1]\n",
    "                r_keypoints = ref_descriptor[2]\n",
    "\n",
    "                matching_range = np.arange(r_descriptors.shape[1])\n",
    "\n",
    "                try:    \n",
    "                    f_nearestneighbor, diff = getMatches(p_descriptors,r_descriptors,2,range_to_match=matching_range)\n",
    "                    diff = diff/np.amax(diff) # max-normalization of differences\n",
    "                    diff_ratio = diff[:,0]/diff[:,1]\n",
    "                    diff_ratios[:,i_r] = diff_ratio\n",
    "                    diff_scores[:,i_r] = diff\n",
    "                    diff_indexs[:,i_r] = f_nearestneighbor\n",
    "\n",
    "                    # Taking note of the matched keypoints\n",
    "                    local_keypoint_matches.append([\n",
    "                        obj_meta,\n",
    "                        p_keypoints,\n",
    "                        r_keypoints[f_nearestneighbor[:,0]]\n",
    "                    ])\n",
    "\n",
    "                except Exception as ex:\n",
    "                    print(rotation,\"Error Matching:\",ex)\n",
    "\n",
    "            # Accumulating the diff_ratio matrix for every partial (rotated) object\n",
    "            g_iteration_scores.append([\n",
    "                obj_meta,\n",
    "                diff_ratios,\n",
    "                diff_indexs,\n",
    "                diff_scores,\n",
    "                local_keypoint_matches\n",
    "            ])\n",
    "\n",
    "            if release_count % 2 == 0:\n",
    "                #print('Test')\n",
    "                print(\"  radius = {} [G]: Done with {} releases ({}) ({} samples). Time to match {:.3f} seconds. ({})\".format(\n",
    "                    radius,\n",
    "                    len(g_iteration_scores),\n",
    "                    release_count,\n",
    "                    len(g_successive_scores),\n",
    "                    time.time()-t0,\n",
    "                    growing_p_point_cloud.shape\n",
    "                )\n",
    "                     )\n",
    "                t0 = time.time()\n",
    "\n",
    "                current_errors = NN_matcher(g_iteration_scores)\n",
    "                print(\"   G_Error Rate:\",np.sum(current_errors[:,1]/len(g_iteration_scores)))\n",
    "\n",
    "        iteration_errors = NN_matcher(iteration_scores)\n",
    "\n",
    "        g_iteration_errors = NN_matcher(g_iteration_scores)\n",
    "\n",
    "        if len(successive_scores) % 5 == (samples-1)%5 :\n",
    "            try:\n",
    "                print(radius,len(successive_scores),\"Error Rate:\",np.sum(iteration_errors[:,1]/len(iteration_scores)))\n",
    "                print(radius,len(g_successive_scores),\"G_Error Rate:\",np.sum(g_iteration_errors[:,1]/len(g_iteration_scores)))\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "        successive_scores.append([\n",
    "            obj_,\n",
    "            iteration_scores\n",
    "        ])\n",
    "        \n",
    "        successive_errors.append([\n",
    "            obj_,\n",
    "            iteration_errors\n",
    "        ])\n",
    "        \n",
    "        g_successive_scores.append([\n",
    "            obj_,\n",
    "            g_iteration_scores\n",
    "        ])\n",
    "        \n",
    "        g_successive_errors.append([\n",
    "            obj_,\n",
    "            g_iteration_errors\n",
    "        ])\n",
    "    \n",
    "        with open('testing_results/successive/radius_{}_RAW_successive_scores.pickle'.format(radius), 'wb') as f:\n",
    "            pickle.dump(successive_scores,f)\n",
    "\n",
    "        with open('testing_results/successive/radius_{}_RAW_successive_errors.pickle'.format(radius), 'wb') as f:\n",
    "            pickle.dump(successive_errors,f)\n",
    "\n",
    "        with open('testing_results/successive/radius_{}_RANSAC_successive_scores.pickle'.format(radius), 'wb') as f:\n",
    "            pickle.dump(g_successive_scores,f)\n",
    "\n",
    "        with open('testing_results/successive/radius_{}_RANSAC_successive_errors.pickle'.format(radius), 'wb') as f:\n",
    "            pickle.dump(g_successive_errors,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Results of the successive case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "succ_RawNN_errors = []\n",
    "succ_RawNN_partial_errors = []\n",
    "\n",
    "succ_RansacGeneralizedNN_errors = []\n",
    "succ_RansacGeneralizedNN_partial_errors = []\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "for radius in radius_range:\n",
    "    \n",
    "    succ_RawNN_per_iteration_errors = []\n",
    "    succ_RansacGeneralizedNN_per_iteration_errors = []\n",
    "\n",
    "    try:\n",
    "                \n",
    "        with open('testing_results/successive/radius_{}_RAW_successive_scores.pickle'.format(radius), 'rb') as f:\n",
    "            successive_scores = pickle.load(f)\n",
    "\n",
    "        with open('testing_results/successive/radius_{}_RAW_successive_errors.pickle'.format(radius), 'rb') as f:\n",
    "            successive_errors = pickle.load(f)\n",
    "        \n",
    "        for obj_, iteration_errors in successive_errors:    \n",
    "            #print(\"  RAW\",radius,iteration_errors.shape)\n",
    "\n",
    "            if iteration_errors.shape[0] < int(releases/skip):\n",
    "                continue\n",
    "            else:\n",
    "                succ_RawNN_per_iteration_errors.append(iteration_errors[:int(releases/skip)])\n",
    "       \n",
    "        succ_RawNN_errors.append([\n",
    "            radius,\n",
    "            np.asarray(succ_RawNN_per_iteration_errors)\n",
    "        ])\n",
    "        \n",
    "        print(np.asarray(succ_RawNN_per_iteration_errors).shape)\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "                \n",
    "        with open('testing_results/successive/radius_{}_RANSAC_successive_scores.pickle'.format(radius), 'rb') as f:\n",
    "            successive_scores = pickle.load(f)\n",
    "\n",
    "        with open('testing_results/successive/radius_{}_RANSAC_successive_errors.pickle'.format(radius), 'rb') as f:\n",
    "            successive_errors = pickle.load(f)\n",
    "        \n",
    "        for obj_, iteration_scores in successive_scores:#[:-1]:    \n",
    "            #print(\"  RANSAC\",radius,iteration_errors.shape)\n",
    "            iteration_errors = NN_matcher(iteration_scores)\n",
    "\n",
    "            if iteration_errors.shape[0] < int(releases/skip):\n",
    "                continue\n",
    "            else:\n",
    "                succ_RansacGeneralizedNN_per_iteration_errors.append(iteration_errors[:int(releases/skip)])\n",
    "       \n",
    "        succ_RansacGeneralizedNN_errors.append([\n",
    "            radius,\n",
    "            np.asarray(succ_RansacGeneralizedNN_per_iteration_errors)\n",
    "        ])\n",
    "        \n",
    "        print(np.asarray(succ_RansacGeneralizedNN_errors).shape)\n",
    "\n",
    "    except:# Exception as ex:\n",
    "        #print(radius,\": successive RansacNN\\n  \", ex)\n",
    "        pass\n",
    "    \n",
    "    print(\"Done with radius = {:.2f} in {:.3f} seconds\".format(radius,time.time() - t0))\n",
    "    t0 = time.time()\n",
    "    \n",
    "for radius, per_iteration_errors in succ_RawNN_errors:\n",
    "\n",
    "    #print(per_iteration_errors.shape)\n",
    "\n",
    "    succ_RawNN_partial_errors_per_rel = []\n",
    "\n",
    "    for rel_i in np.arange(per_iteration_errors.shape[1]):\n",
    "\n",
    "        correct_interspace_labels_idxs = np.where(per_iteration_errors[:,rel_i,1]==0)[0]\n",
    "\n",
    "        intraspace_errors  = per_iteration_errors[correct_interspace_labels_idxs,rel_i,2]\n",
    "\n",
    "        succ_RawNN_partial_errors_per_rel.append([\n",
    "            rel_i,\n",
    "            np.mean(intraspace_errors),\n",
    "            np.std(intraspace_errors)\n",
    "        ])\n",
    "\n",
    "    succ_RawNN_partial_errors.append([\n",
    "        radius,\n",
    "        np.asarray(succ_RawNN_partial_errors_per_rel)\n",
    "    ])\n",
    "    \n",
    "for radius, per_iteration_errors in succ_RansacGeneralizedNN_errors:\n",
    "\n",
    "    #print(radius,per_iteration_errors.shape)\n",
    "\n",
    "    succ_RansacGeneralizedNN_errors_per_rel = []\n",
    "\n",
    "    for rel_i in np.arange(per_iteration_errors.shape[1]):\n",
    "\n",
    "        correct_interspace_labels_idxs = np.where(per_iteration_errors[:,rel_i,1]==0)[0]\n",
    "\n",
    "        intraspace_errors  = per_iteration_errors[correct_interspace_labels_idxs,rel_i,2]\n",
    "\n",
    "        succ_RansacGeneralizedNN_errors_per_rel.append([\n",
    "            rel_i,\n",
    "            np.mean(intraspace_errors),\n",
    "            np.std(intraspace_errors)\n",
    "        ])\n",
    "\n",
    "    succ_RansacGeneralizedNN_partial_errors.append([\n",
    "        radius,\n",
    "        np.asarray(succ_RansacGeneralizedNN_errors_per_rel)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(15, 5))\n",
    "\n",
    "ax1 = fig.add_subplot(121) \n",
    "\n",
    "ax1.grid(alpha = 0.7)\n",
    "ax1.set_ylim(-0.025,1.025)\n",
    "ax1.set_xlim(0,releases-skip)\n",
    "markersize = 8\n",
    "\n",
    "ax1.set_ylabel(\"INTER-space Privacy\")\n",
    "ax1.set_xlabel(\"Releases\")\n",
    "\n",
    "for radius, RawNN_per_iteration_errors in succ_RawNN_errors:\n",
    "    print(RawNN_per_iteration_errors.shape)\n",
    "    ax1.plot(\n",
    "        np.arange(1,releases-skip,skip),#[:RawNN_per_iteration_errors.shape[1]],\n",
    "        np.mean(RawNN_per_iteration_errors[:,:,1], axis = 0), \n",
    "        ':o',\n",
    "        label = \"r =\"+ str(radius) + \" Raw\"\n",
    "    )\n",
    "    \n",
    "for radius, RansacNN_per_iteration_errors in succ_RansacGeneralizedNN_errors:\n",
    "    print(RansacNN_per_iteration_errors.shape)\n",
    "    ax1.plot(\n",
    "        np.arange(1,releases-skip,skip),\n",
    "        np.mean(RansacNN_per_iteration_errors[:,:,1], axis = 0),\n",
    "        '-s',\n",
    "        label = \"r =\"+ str(radius) + \" RANSAC\"\n",
    "    )\n",
    "\n",
    "ax1.legend(loc = \"best\", ncol = 2)\n",
    "\n",
    "ax2 = fig.add_subplot(122) \n",
    "\n",
    "ax2.grid(alpha = 0.7)\n",
    "ax2.set_ylim(-0.25,12.25)\n",
    "ax2.set_xlim(0,releases-skip)\n",
    "\n",
    "ax2.set_ylabel(\"INTRA-space Privacy\")\n",
    "ax2.set_xlabel(\"Partial Radius\")\n",
    "#ax2.set_yticklabels(fontsize = 16)\n",
    "#ax2.set_xticklabels(fontsize = 16)\n",
    "\n",
    "#plt.minorticks_on()\n",
    "\n",
    "for radius, errors_per_rel in succ_RansacGeneralizedNN_partial_errors:\n",
    "    ax2.plot(\n",
    "        np.arange(1,releases-skip,skip),\n",
    "        errors_per_rel[:,1], \n",
    "        #errors_per_rel[:,2],\n",
    "        '-s',\n",
    "        linewidth = 2, #capsize = 4.0, \n",
    "        #marker = markers[0],\n",
    "        #fillstyle = 'none',\n",
    "        mew = 2, markersize = markersize,\n",
    "        label = \"r =\"+ str(radius)+\", RANSAC\"\n",
    "    )\n",
    "\n",
    "ax2.legend(loc = \"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3.1 Testing with conservative plane releasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Parameters:\n",
    "\n",
    "Also, we use the same successive samples from successive releasing for direct comparability of results.\n",
    "\"\"\"\n",
    "# We used a radius range of 0.25 to 5.0 in increments of 0.25.\n",
    "radius_range = radius_range[:2]\n",
    "\n",
    "# For our work, we orignally used 50 samples with further 100 successive releases for our investigation.\n",
    "# Below are lower parameters, change as desired.\n",
    "#samples = 50\n",
    "#releases = 50\n",
    "\n",
    "planes = np.arange(1,30,3)\n",
    "\n",
    "# For demonstration purposes, we skip testing some successive samples but we still accumulate them.\n",
    "skip = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for radius in radius_range:\n",
    "        \n",
    "    t0 = time.time()\n",
    "    \n",
    "    try:\n",
    "        with open('testing_samples/{}_successive_point_cloud.pickle'.format(radius), 'rb') as f:\n",
    "            successive_point_collection = pickle.load(f)\n",
    "            samples = len(successive_point_collection)\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    successive_scores = []\n",
    "    successive_errors = []\n",
    "    \n",
    "    g_successive_scores = []\n",
    "    g_successive_errors = []\n",
    "\n",
    "    for obj_, growing_point_collection in successive_point_collection:\n",
    "        \n",
    "        iteration_scores = []\n",
    "        \n",
    "        g_iteration_scores = []\n",
    "        \n",
    "        # ROTATION param\n",
    "        random_theta =  (2*np.pi)*np.random.random()# from [0, 2pi)\n",
    "        random_axis = np.random.choice(np.arange(0,3))\n",
    "        \n",
    "        # TRANSLATION param\n",
    "        random_tx_axis = np.random.choice(np.arange(0,3))\n",
    "        random_translation = np.random.random()\n",
    "        \n",
    "        growing_point_cloud = []\n",
    "        growing_p_point_cloud = []\n",
    "        growing_p_triangles = []\n",
    "        \n",
    "        releases = len(growing_point_collection)\n",
    "        release_count = 0\n",
    "    \n",
    "        for obj_meta, partial_pointcloud, partial_triangles in growing_point_collection:\n",
    "            #i_obj, released_growing_point_collection in enumerate(growing_point_collection):\n",
    "\n",
    "            rotated_pointCloud = rotatePointCloud(partial_pointcloud, random_theta, random_axis)\n",
    "\n",
    "            # TRANSLATION\n",
    "            t_pointCloud = np.asarray(rotated_pointCloud)\n",
    "            t_pointCloud[:,random_tx_axis] = t_pointCloud[:,random_tx_axis] + random_translation\n",
    "            t_triangles = np.asarray(partial_triangles)#-vertices_length\n",
    "            \n",
    "            #Regular Accumulation\n",
    "            if len(growing_point_cloud) == 0:\n",
    "                growing_point_cloud = t_pointCloud\n",
    "\n",
    "            else:\n",
    "                growing_point_cloud = np.concatenate(\n",
    "                    (growing_point_cloud,t_pointCloud),\n",
    "                    axis=0\n",
    "                )\n",
    "                \n",
    "            #RANSAC generalizations\n",
    "            if len(growing_p_point_cloud) == 0:\n",
    "                gen_planes = getLOCALIZEDRansacPlanes(\n",
    "                    pointCloud = t_pointCloud,\n",
    "                    original_vertex = obj_meta[-1]\n",
    "                )\n",
    "            else:\n",
    "                gen_planes = updatePlanesWithSubsumption(\n",
    "                    new_pointCloud=t_pointCloud,\n",
    "                    existing_pointCloud=growing_p_point_cloud,\n",
    "                    planes_to_find = max(min(release_count,50),30),\n",
    "                    #verbose=True\n",
    "                )\n",
    "            \n",
    "            if len(gen_planes) == 0:\n",
    "                print(\"No gen planes after release\",release_count,growing_point_cloud.shape)\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                updated_point_cloud, updated_triangles = getGeneralizedPointCloud(\n",
    "                    planes = gen_planes,\n",
    "                    triangle_area_threshold = 0.2,#2.0*np.amax(getTriangleAreas(partial_pointCloud, partial_triangles))\n",
    "                    #verbose = True\n",
    "                )\n",
    "                growing_p_point_cloud = updated_point_cloud\n",
    "                growing_p_triangles = updated_triangles\n",
    "                \n",
    "                #print(\" Successful:\",release_count,len(growing_p_point_cloud), len(growing_p_triangles),partial_pointCloud.shape)\n",
    "            except Exception as ex:\n",
    "                print(\"Error getting updated point cloud in release\",release_count+1)\n",
    "                print(\" \",growing_p_point_cloud.shape, growing_p_triangles.shape,partial_pointCloud.shape)\n",
    "                #print(ex)\n",
    "                continue\n",
    "                \n",
    "            if len(growing_p_point_cloud) == 0:\n",
    "                continue\n",
    "                \n",
    "            release_count += 1\n",
    "            \n",
    "            if release_count % skip != 1: # skip \n",
    "                continue\n",
    "                \n",
    "            #Regular Processing\n",
    "            growing_point_cloud = np.unique(growing_point_cloud,axis=0)\n",
    "            \n",
    "            try:\n",
    "                p_descriptors, p_keypoints, p_d_c = getSpinImageDescriptors(\n",
    "                    growing_point_cloud,\n",
    "                    down_resolution = 5,\n",
    "                    cylindrical_quantization = [4,5]\n",
    "                )\n",
    "            except:\n",
    "                p_descriptors = []\n",
    "                p_keypoints = []\n",
    "\n",
    "            # Resetting the diff_Ratio matrix\n",
    "            diff_scores = np.ones((p_descriptors.shape[0],len(descriptors),2))\n",
    "            diff_ratios = np.ones((p_descriptors.shape[0],len(descriptors)))\n",
    "            diff_indexs = np.ones((p_descriptors.shape[0],len(descriptors),2))\n",
    "\n",
    "            local_keypoint_matches = []\n",
    "\n",
    "            for i_r, ref_descriptor in enumerate(descriptors):\n",
    "\n",
    "                r_descriptors = ref_descriptor[1]\n",
    "                r_keypoints = ref_descriptor[2]\n",
    "\n",
    "                matching_range = np.arange(r_descriptors.shape[1])\n",
    "\n",
    "                try:    \n",
    "                    f_nearestneighbor, diff = getMatches(p_descriptors,r_descriptors,2,range_to_match=matching_range)\n",
    "                    diff = diff/np.amax(diff) # max-normalization of differences\n",
    "                    diff_ratio = diff[:,0]/diff[:,1]\n",
    "                    diff_ratios[:,i_r] = diff_ratio\n",
    "                    diff_scores[:,i_r] = diff\n",
    "                    diff_indexs[:,i_r] = f_nearestneighbor\n",
    "\n",
    "                    # Taking note of the matched keypoints\n",
    "                    local_keypoint_matches.append([\n",
    "                        obj_meta,\n",
    "                        p_keypoints,\n",
    "                        r_keypoints[f_nearestneighbor[:,0]]\n",
    "                    ])\n",
    "\n",
    "                except Exception as ex:\n",
    "                    print(rotation,\"Error Matching:\",ex)\n",
    "\n",
    "            # Accumulating the diff_ratio matrix for every partial (rotated) object\n",
    "            iteration_scores.append([\n",
    "                obj_meta,\n",
    "                diff_ratios,\n",
    "                diff_indexs,\n",
    "                diff_scores,\n",
    "                local_keypoint_matches\n",
    "            ])\n",
    "\n",
    "            if release_count % 2 == 0:\n",
    "                #print('Test')\n",
    "                print(\"  radius = {}: Done with {} releases ({}) ({} samples). Time to match {:.3f} seconds. ({})\".format(\n",
    "                    radius,\n",
    "                    len(iteration_scores),\n",
    "                    release_count,\n",
    "                    len(successive_scores),\n",
    "                    time.time()-t0,\n",
    "                    growing_point_cloud.shape\n",
    "                )\n",
    "                     )\n",
    "                t0 = time.time()\n",
    "\n",
    "                current_errors = NN_matcher(iteration_scores)\n",
    "                print(\"   Error Rate:\",np.sum(current_errors[:,1]/len(iteration_scores)))\n",
    "            \n",
    "            #RANSAC Processing         \n",
    "            try:\n",
    "                p_descriptors, p_keypoints, p_d_c = getSpinImageDescriptors(\n",
    "                    growing_p_point_cloud,\n",
    "                    down_resolution = 5,\n",
    "                    cylindrical_quantization = [4,5]\n",
    "                )\n",
    "            except:\n",
    "                p_descriptors = []\n",
    "                p_keypoints = []\n",
    "                \n",
    "                print(\"Error getting descriptors at release\",release_count,\"; using next release as this release.\")\n",
    "                \n",
    "                #release_count -= 1\n",
    "\n",
    "                continue\n",
    "\n",
    "            # Resetting the diff_Ratio matrix\n",
    "            diff_scores = np.ones((p_descriptors.shape[0],len(descriptors),2))\n",
    "            diff_ratios = np.ones((p_descriptors.shape[0],len(descriptors)))\n",
    "            diff_indexs = np.ones((p_descriptors.shape[0],len(descriptors),2))\n",
    "\n",
    "            local_keypoint_matches = []\n",
    "\n",
    "            for i_r, ref_descriptor in enumerate(descriptors):\n",
    "\n",
    "                #o_, r_descriptors, r_keypoints, r_d_c\n",
    "                r_descriptors = ref_descriptor[1]\n",
    "                r_keypoints = ref_descriptor[2]\n",
    "\n",
    "                matching_range = np.arange(r_descriptors.shape[1])\n",
    "\n",
    "                try:    \n",
    "                    f_nearestneighbor, diff = getMatches(p_descriptors,r_descriptors,2,range_to_match=matching_range)\n",
    "                    diff = diff/np.amax(diff) # max-normalization of differences\n",
    "                    diff_ratio = diff[:,0]/diff[:,1]\n",
    "                    diff_ratios[:,i_r] = diff_ratio\n",
    "                    diff_scores[:,i_r] = diff\n",
    "                    diff_indexs[:,i_r] = f_nearestneighbor\n",
    "\n",
    "                    # Taking note of the matched keypoints\n",
    "                    local_keypoint_matches.append([\n",
    "                        obj_meta,\n",
    "                        p_keypoints,\n",
    "                        r_keypoints[f_nearestneighbor[:,0]]\n",
    "                    ])\n",
    "\n",
    "                except Exception as ex:\n",
    "                    print(rotation,\"Error Matching:\",ex)\n",
    "\n",
    "            # Accumulating the diff_ratio matrix for every partial (rotated) object\n",
    "            g_iteration_scores.append([\n",
    "                obj_meta,\n",
    "                diff_ratios,\n",
    "                diff_indexs,\n",
    "                diff_scores,\n",
    "                local_keypoint_matches\n",
    "            ])\n",
    "\n",
    "            if release_count % 2 == 0:\n",
    "                #print('Test')\n",
    "                print(\"  radius = {} [G]: Done with {} releases ({}) ({} samples). Time to match {:.3f} seconds. ({})\".format(\n",
    "                    radius,\n",
    "                    len(g_iteration_scores),\n",
    "                    release_count,\n",
    "                    len(g_successive_scores),\n",
    "                    time.time()-t0,\n",
    "                    growing_p_point_cloud.shape\n",
    "                )\n",
    "                     )\n",
    "                t0 = time.time()\n",
    "\n",
    "                current_errors = NN_matcher(g_iteration_scores)\n",
    "                print(\"   G_Error Rate:\",np.sum(current_errors[:,1]/len(g_iteration_scores)))\n",
    "\n",
    "        iteration_errors = NN_matcher(iteration_scores)\n",
    "\n",
    "        g_iteration_errors = NN_matcher(g_iteration_scores)\n",
    "\n",
    "        if len(successive_scores) % 5 == (samples-1)%5 :\n",
    "            try:\n",
    "                print(radius,len(successive_scores),\"Error Rate:\",np.sum(iteration_errors[:,1]/len(iteration_scores)))\n",
    "                print(radius,len(g_successive_scores),\"G_Error Rate:\",np.sum(g_iteration_errors[:,1]/len(g_iteration_scores)))\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "        successive_scores.append([\n",
    "            obj_,\n",
    "            iteration_scores\n",
    "        ])\n",
    "        \n",
    "        successive_errors.append([\n",
    "            obj_,\n",
    "            iteration_errors\n",
    "        ])\n",
    "        \n",
    "        g_successive_scores.append([\n",
    "            obj_,\n",
    "            g_iteration_scores\n",
    "        ])\n",
    "        \n",
    "        g_successive_errors.append([\n",
    "            obj_,\n",
    "            g_iteration_errors\n",
    "        ])\n",
    "    \n",
    "        with open('testing_results/successive/radius_{}_RAW_successive_scores.pickle'.format(radius), 'wb') as f:\n",
    "            pickle.dump(successive_scores,f)\n",
    "\n",
    "        with open('testing_results/successive/radius_{}_RAW_successive_errors.pickle'.format(radius), 'wb') as f:\n",
    "            pickle.dump(successive_errors,f)\n",
    "\n",
    "        with open('testing_results/successive/radius_{}_RANSAC_successive_scores.pickle'.format(radius), 'wb') as f:\n",
    "            pickle.dump(g_successive_scores,f)\n",
    "\n",
    "        with open('testing_results/successive/radius_{}_RANSAC_successive_errors.pickle'.format(radius), 'wb') as f:\n",
    "            pickle.dump(g_successive_errors,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 Results with conservative plane releasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conservative_RANSAC_error_results = []\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "for radius in radius_range:\n",
    "    \n",
    "    succ_RansacGeneralizedNN_per_iteration_errors = []\n",
    "    \n",
    "    try:\n",
    "                \n",
    "        with open('testing_results/conservative/radius_{}_RANSAC_conservative_scores.pickle'.format(radius), 'rb') as f:\n",
    "            conservative_scores = pickle.load(f)\n",
    "\n",
    "        with open('testing_results/conservative/radius_{}_RANSAC_conservative_errors.pickle'.format(radius), 'rb') as f:\n",
    "            conservative_errors = pickle.load(f)\n",
    "        \n",
    "        for obj_, per_plane_scores in successive_scores:#[:-1]:  \n",
    "            \n",
    "            per_plane_errors = []\n",
    "            \n",
    "            skip = False\n",
    "            \n",
    "            for planes, iteration_scores in per_plane_scores:\n",
    "                #print(iteration_errors.shape)\n",
    "                \n",
    "                iteration_errors = NN_matcher(iteration_scores)\n",
    "                \n",
    "                #print(planes,iteration_errors.shape)\n",
    "\n",
    "                if iteration_errors.shape[0] >=18:\n",
    "                    per_plane_errors.append(iteration_errors[:18])\n",
    "                else:\n",
    "                    skip = True\n",
    "                    #print(\"RANSAC: skipped\",iteration_errors.shape)\n",
    "                    \n",
    "            if not skip:\n",
    "                succ_RansacGeneralizedNN_per_iteration_errors.append(per_plane_errors)\n",
    "       \n",
    "        conservative_RANSAC_error_results.append([\n",
    "            radius,\n",
    "            succ_RansacGeneralizedNN_per_iteration_errors\n",
    "        ])\n",
    "              \n",
    "    except Exception as ex:\n",
    "        print(radius,\": conservative RansacNN\\n  \", ex)\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    print(\"Done with radius = {:.2f} in {:.3f} seconds\".format(radius,time.time() - t0))\n",
    "    t0 = time.time()\n",
    "    \n",
    "\"\"\"\n",
    "# Uncomment below if you want to overwrite the existing results.\n",
    "\"\"\"\n",
    "with open('testing_results/conservative/conservative_RANSAC_error_results.pickle', 'wb') as f:\n",
    "    pickle.dump(conservative_RANSAC_error_results,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Preparing the results of the case with *Conservative Releasing*.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "releases_range = np.arange(1,releases-skip,skip)\n",
    "\n",
    "X, Y = np.meshgrid(releases_range, planes)\n",
    "\n",
    "test_vp_cn_05 = np.asarray(conservative_RANSAC_error_results[0][1])\n",
    "mean_vp_cn_05 = np.mean(test_vp_cn_05[:,:,:,1],axis = 0)\n",
    "\n",
    "test_vp_cn_10 = np.asarray(conservative_RANSAC_error_results[1][1])\n",
    "mean_vp_cn_05 = np.mean(test_vp_cn_05[:,:,:,1],axis = 0)\n",
    "\n",
    "intra_vp_cn_10 = np.zeros(test_vp_cn_10.shape[1:])\n",
    "intra_vp_cn_05 = np.zeros(test_vp_cn_05.shape[1:])\n",
    "\n",
    "for plane_i, plane in enumerate(planes):\n",
    "        \n",
    "    for rel_i, rel in enumerate(releases_range):\n",
    "    \n",
    "        correct_interspace_labels_idxs_05 = np.where(test_vp_cn_05[:,plane_i,rel_i,1]==0)[0]\n",
    "        correct_interspace_labels_idxs_10 = np.where(test_vp_cn_10[:,plane_i,rel_i,1]==0)[0]\n",
    "\n",
    "        intraspace_errors_05  = test_vp_cn_05[correct_interspace_labels_idxs_05,plane_i,rel_i,2]\n",
    "        intraspace_errors_10  = test_vp_cn_10[correct_interspace_labels_idxs_10,plane_i,rel_i,2]\n",
    "        \n",
    "        intra_vp_cn_05[plane_i,rel_i] = np.asarray([\n",
    "            np.mean(intraspace_errors_05),\n",
    "            np.std(intraspace_errors_05),\n",
    "            0,\n",
    "            np.nan\n",
    "        ])\n",
    "        \n",
    "        intra_vp_cn_10[plane_i,rel_i] = np.asarray([\n",
    "            np.mean(intraspace_errors_10),\n",
    "            np.std(intraspace_errors_10),\n",
    "            0,\n",
    "            np.nan\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(11,8))\n",
    "ax = plt.axes(projection='3d')\n",
    "\n",
    "surf = ax.plot_surface(\n",
    "    X, Y, \n",
    "    mean_vp_cn_05, \n",
    "    cmap=plt.cm.plasma,\n",
    ")\n",
    "surf.set_clim(0.0,1.0)\n",
    "\n",
    "ax.set_title(\"r = 0.5\", fontsize = 24)\n",
    "ax.set_xlabel(\"Releases\", labelpad=10, fontsize = 24)\n",
    "ax.set_xlim(0,100)\n",
    "ax.set_xticklabels(np.arange(0,101,20),fontsize = 16)\n",
    "ax.set_zlabel(\"INTER-space Privacy\", labelpad=10, fontsize = 24)\n",
    "ax.set_zlim(0,1)\n",
    "ax.set_zticklabels([0,0.2,0.4,0.6,0.8,1.0],fontsize = 16)\n",
    "ax.set_ylabel(\"Max number of planes\", labelpad=10, fontsize = 22)#, offset = 1)\n",
    "ax.set_ylim(0,30)\n",
    "ax.set_yticklabels(np.arange(0,35,5),fontsize = 16)\n",
    "\n",
    "cbar = fig.colorbar(surf, aspect=30, ticks = np.arange(0.0,1.1,0.25))\n",
    "cbar.ax.set_yticklabels(np.arange(0.0,1.1,0.25),fontsize = 16)\n",
    "\n",
    "ax.view_init(25,135);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-3d_env] *",
   "language": "python",
   "name": "conda-env-.conda-3d_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
