{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import scipy.io\n",
    "import time\n",
    "import h5py\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n",
    "from mpl_toolkits.axes_grid1.axes_divider import make_axes_locatable\n",
    "\n",
    "from numpy import linalg as LA\n",
    "from scipy.spatial import Delaunay\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "#sys.path.insert(0, \"../\")\n",
    "from info3d import *\n",
    "from nn_matchers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global parameters\n",
    "\n",
    "radius_range = np.arange(0.5,1.6,0.5)\n",
    "\n",
    "with open('point_collection/new_contiguous_point_collection.pickle','rb') as f: \n",
    "    new_contiguous_point_collection = pickle.load(f)\n",
    "\n",
    "point_collection_indices = np.arange(len(new_contiguous_point_collection))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0: Extract the point cloud data from the OBJ files\n",
    " - store as numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the path to the Raw point clouds\n",
    "\n",
    "OBJ_DIR = 'point_clouds/'\n",
    "\n",
    "# Format is a tuple of (file path, file name).\n",
    "OBJ_PATHS = [ [os.path.join(OBJ_DIR, f), f]\n",
    "                    for f in os.listdir(OBJ_DIR) ]\n",
    "\n",
    "OBJ_PATHS.remove(OBJ_PATHS[0]) # remove .DS Store file\n",
    "\n",
    "# If we desire to treat the various objects within the OBJ as different spaces,\n",
    "# we store it here.\n",
    "new_per_object_collection = []\n",
    "\n",
    "# If we desire to treat the entire OBJ as a space,\n",
    "# we store it here.\n",
    "new_per_space_collection = []\n",
    "\n",
    "# This is to combine the objects within an OBJ to slightly larger objects. \n",
    "#Roughly 2.5m radius\n",
    "combining_radius = 2.5\n",
    "\n",
    "for path, filename in OBJ_PATHS:\n",
    "    \n",
    "    pointCloudFile = open(path)#'point_cloud.obj')('SpatialMesh.obj')#\n",
    "\n",
    "    pointCloudLines = pointCloudFile.readlines()\n",
    "    \n",
    "    pointCollection = [] # object_number, vertices, vertex normals, polygons\n",
    "\n",
    "    pointCollection_no_objects = [[],[],[]] \n",
    "    prev_length = 0\n",
    "    vertices_length = 1\n",
    "    \n",
    "    for line in pointCloudLines:\n",
    "\n",
    "        if line == '\\n': continue\n",
    "\n",
    "        line_items = line.split()\n",
    "\n",
    "        if line_items[0] == 'o':\n",
    "            object_item = line_items[1].split('.')\n",
    "            object_number = int(object_item[-1])\n",
    "            pointCollection.append([object_number, [], [], []])\n",
    "            vertices_length += prev_length\n",
    "\n",
    "            #print(object_number)\n",
    "\n",
    "        if line_items[0] == 'v':\n",
    "            x = line_items[1]\n",
    "            y = line_items[2]\n",
    "            z = line_items[3]\n",
    "            if len(line_items)> 4:\n",
    "                c = line_items[4]\n",
    "                if float(c) > 0.85:\n",
    "                    pointCollection[object_number-1][1].append([float(x),float(y),float(z),float(c)])\n",
    "                    pointCollection_no_objects[0].append([float(x),float(y),float(z),float(c)])\n",
    "            else:\n",
    "                pointCollection[object_number-1][1].append([float(x),float(y),float(z)])\n",
    "                pointCollection_no_objects[0].append([float(x),float(y),float(z)])\n",
    "            prev_length = len(pointCollection[object_number-1][1])\n",
    "\n",
    "\n",
    "        if line_items[0] == 'vn':\n",
    "            x = line_items[1]\n",
    "            y = line_items[2]\n",
    "            z = line_items[3]\n",
    "            pointCollection[object_number-1][2].append([float(x),float(y),float(z)])\n",
    "            pointCollection_no_objects[1].append([float(x),float(y),float(z)])\n",
    "\n",
    "        if line_items[0] == 'f':\n",
    "            p1 = int(line_items[1].split('//')[0])\n",
    "            p2 = int(line_items[2].split('//')[0])\n",
    "            p3 = int(line_items[3].split('//')[0])\n",
    "            pointCollection[object_number-1][3].append([p1-vertices_length,\n",
    "                                                        p2-vertices_length,\n",
    "                                                        p3-vertices_length])\n",
    "            pointCollection_no_objects[2].append([p1-1,p2-1,p3-1])        \n",
    "\n",
    "    pointCloudFile.close\n",
    "    print(filename)\n",
    "    #print(\" Length of point collection\",len(pointCollection[1][1]))\n",
    "    print(\" Total number of point collections\",len(pointCollection))\n",
    "    \n",
    "    #Combining objects: 1. Getting centroids\n",
    "    centroids = []\n",
    "\n",
    "    for object_name, pointCloud, _vn, triangles in pointCollection:\n",
    "        pointCloud = np.asarray(pointCloud)\n",
    "        triangles = np.asarray(triangles)#-vertices_length\n",
    "        normals = np.asarray(_vn)\n",
    "        \n",
    "        centroids.append([object_name,\n",
    "              np.mean(pointCloud[:,0]), # x-axis\n",
    "              np.mean(pointCloud[:,2]), # z-axis\n",
    "             ])\n",
    "        \n",
    "    centroids = np.asarray(centroids)\n",
    "    remaining_centroids = np.copy(centroids)\n",
    "    \n",
    "    combinedPointCollection = []\n",
    "    count = 1\n",
    "    \n",
    "    while(remaining_centroids.size!=0):\n",
    "        #print(\"  \",remaining_centroids[0,0],remaining_centroids.shape)\n",
    "        #focusing on the first remaining collection, i.e. through its centroid\n",
    "        collection = np.where(LA.norm(remaining_centroids[:,1:] - remaining_centroids[0,1:], axis = 1)<combining_radius)[0]\n",
    "        \n",
    "        #print(\"  \",collection)\n",
    "        \n",
    "        pointCloud = np.asarray(pointCollection[int(remaining_centroids[collection[0],0]-1)][1])\n",
    "        normals = np.asarray(pointCollection[int(remaining_centroids[collection[0],0])-1][2])\n",
    "        triangles = np.asarray(pointCollection[int(remaining_centroids[collection[0],0])-1][3])\n",
    "\n",
    "        for index in collection[1:]:\n",
    "            \n",
    "            n_pointCloud = np.asarray(pointCollection[int(remaining_centroids[index,0])-1][1])\n",
    "            n_normals = np.asarray(pointCollection[int(remaining_centroids[index,0])-1][2])\n",
    "            n_triangles = np.asarray(pointCollection[int(remaining_centroids[index,0])-1][3])+len(pointCloud)\n",
    "\n",
    "            pointCloud = np.concatenate((pointCloud,n_pointCloud),0)\n",
    "            normals = np.concatenate((normals,n_normals),0)\n",
    "            triangles = np.concatenate((triangles,n_triangles),0)\n",
    "        \n",
    "        new_per_object_collection.append([\n",
    "            filename+str(count),\n",
    "            pointCloud,\n",
    "            normals,\n",
    "            triangles])\n",
    "        \n",
    "        combinedPointCollection.append([\n",
    "            pointCloud,\n",
    "            normals,\n",
    "            triangles])\n",
    "            \n",
    "        count += 1\n",
    "        remaining_centroids = np.delete(remaining_centroids,collection,axis=0)\n",
    "        #print(\"   post\",remaining_centroids.shape)\n",
    "        \n",
    "    print(\" Resulting number =\",len(combinedPointCollection))\n",
    "#    new_per_object_collection.append([\n",
    "#        filename,\n",
    "#        combinedPointCollection\n",
    "#    ])\n",
    "    \n",
    "    new_per_space_collection.append([\n",
    "        filename,\n",
    "        np.asarray(pointCollection_no_objects[0]),\n",
    "        np.asarray(pointCollection_no_objects[1]),\n",
    "        np.asarray(pointCollection_no_objects[2])\n",
    "    ])\n",
    "\n",
    "print(len(new_per_object_collection),\"total objects.\")\n",
    "print(len(new_per_space_collection),\"total spaces.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining the point position information with their normal vectors.\n",
    "\n",
    "new_contiguous_point_collection = []\n",
    "\n",
    "for object_name, pointCloud, vn, triangles in new_per_space_collection:\n",
    "    new_contiguous_point_collection.append([\n",
    "        object_name, \n",
    "        np.concatenate((pointCloud,vn),axis = 1),\n",
    "        triangles\n",
    "    ])\n",
    "    \n",
    "sample_index = np.random.choice(len(new_contiguous_point_collection))\n",
    "print(\"Sample:\",\n",
    "      new_contiguous_point_collection[sample_index][0],\n",
    "      new_contiguous_point_collection[sample_index][1].shape, # shape of the point cloud array\n",
    "      new_contiguous_point_collection[sample_index][2].shape # shape of the triangle array\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment below if you want to overwrite the exisiting point collection.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "with open('point_collection/new_contiguous_point_collection.pickle','wb') as f: \n",
    "    pickle.dump(new_contiguous_point_collection,f)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "fig=plt.figure()#figsize=(20, 14.5))\n",
    "\n",
    "sample_name = new_contiguous_point_collection[sample_index][0]\n",
    "sample_pointCloud = new_contiguous_point_collection[sample_index][1]\n",
    "sample_triangles = new_contiguous_point_collection[sample_index][2]\n",
    "\n",
    "print(sample_name,sample_pointCloud.shape,sample_triangles.shape)\n",
    "\n",
    "ax0 = fig.add_subplot(1,1,1, projection='3d')\n",
    "ax0.set_xlabel('X')\n",
    "ax0.set_ylabel('Z')\n",
    "ax0.set_zlabel('Y')\n",
    "ax0.set_zlim(-1.5,1)\n",
    "#ax0.set_title('Dense Point Cloud')\n",
    "\n",
    "X = sample_pointCloud[:,0]\n",
    "Y = sample_pointCloud[:,1]\n",
    "Z = sample_pointCloud[:,2]\n",
    "\n",
    "ax0.set_xlim(min(X), max(X))\n",
    "ax0.set_ylim(min(-Z), max(-Z))\n",
    "\n",
    "try:\n",
    "    ax0.scatter(\n",
    "        X,-Z,Y,\n",
    "        alpha = 0.3\n",
    "    )\n",
    "\n",
    "    ax0.plot_trisurf(\n",
    "        X, -Z, Y, \n",
    "        triangles=sample_triangles,\n",
    "        cmap=plt.cm.Spectral\n",
    "        #alpha = 0.5\n",
    "    )\n",
    "\n",
    "except Exception as ex:\n",
    "    print(ex)\n",
    "    pass\n",
    "\n",
    "#plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0.1 Getting the NNs for creating samples for testing\n",
    "\n",
    "NOTE! This step is only essential for creating sample sapces for testing. There are prepared samples at testing_samples/*. Thus, you may skip this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For better testing, use bigger nn_range. \n",
    "# Originally, we used nn_range = 20,000 to reliably produce partial spaces of up to 3 meters.\n",
    "\n",
    "nn_range = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Create target Directory\n",
    "    os.mkdir('hd5')\n",
    "    print(\"Directory \" , 'hd5' ,  \" Created \") \n",
    "except FileExistsError:\n",
    "    print(\"Directory \" , 'hd5' ,  \" already exists\")\n",
    "\n",
    "for object_name, pointCloud, triangles in new_contiguous_point_collection:\n",
    "    \n",
    "    t0 = time.time()\n",
    "    \n",
    "    print(object_name, np.asarray(pointCloud).shape, np.asarray(triangles).shape)\n",
    "    \n",
    "    nbrs = NearestNeighbors(n_neighbors=min(nn_range, len(pointCloud)-1), algorithm='kd_tree').fit(pointCloud[:,:3])\n",
    "    print(\"  Done in getting tree {:.3f} seconds.\".format(time.time()-t0))\n",
    "\n",
    "    distances, indices = nbrs.kneighbors(pointCloud[:,:3])\n",
    "\n",
    "    print(\"  NN shape: {}, Mean fartherst NN distance = {:3f} m (±{:3f})\".format(\n",
    "        distances.shape,\n",
    "        np.mean(np.max(distances,axis=1)),\n",
    "        np.std(np.max(distances,axis=1))\n",
    "    ))\n",
    "    \n",
    "    with h5py.File('hd5/{}_{}_nn.h5'.format(object_name, nn_range), 'w') as f:\n",
    "        f.create_dataset('distances', data=distances)\n",
    "        f.create_dataset('indices', data=indices)\n",
    "\n",
    "    print(\"  Done in getting {}nn {:.3f} seconds.\".format(nn_range,time.time()-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These created NNs is only reliable up to partial spaces of size 0.922m (±0.100).\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Checking the maximum reliable radius of partial spaces that can be produced with the current NNs.\n",
    "\"\"\"\n",
    "\n",
    "range_of_radii = []\n",
    "\n",
    "for object_name, pointCloud, triangles in new_contiguous_point_collection:\n",
    "\n",
    "    try:\n",
    "        with h5py.File('hd5/{}_{}_nn.h5'.format(object_name,nn_range), 'r') as f:\n",
    "            distances = f['distances'][:]\n",
    "\n",
    "    except:\n",
    "        print(\"Didn't get HD5 file for\", object_name,nn_range)\n",
    "        continue\n",
    "\n",
    "    #extracted_distances = np.copy(distances)\n",
    "    #print(np.mean(distances[:,-1]),np.std(distances[:,-1]))\n",
    "    \n",
    "    range_of_radii.append([\n",
    "        np.mean(distances[:,-1]),\n",
    "        np.std(distances[:,-1])\n",
    "    ])\n",
    "\n",
    "range_of_radii = np.asarray(range_of_radii)\n",
    "\n",
    "print(\"These created NNs is only reliable up to partial spaces of size {:.3f}m (±{:.3f}).\".format(\n",
    "    np.mean(range_of_radii[:,0]),\n",
    "    np.std(range_of_radii[:,0])\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reception-Data61-L5.obj (40491, 6) (67174, 3)\n",
      "  Done extracting from file in 0.045 seconds.\n",
      "   (40491, 1000) (40491, 1000)\n",
      "  Done copying in 0.533 seconds.\n",
      "Driveway.obj (56926, 6) (88560, 3)\n",
      "  Done extracting from file in 0.001 seconds.\n",
      "   (56926, 1000) (56926, 1000)\n",
      "  Done copying in 0.552 seconds.\n",
      "Apartment.obj (72859, 6) (126700, 3)\n",
      "  Done extracting from file in 0.001 seconds.\n",
      "   (72859, 1000) (72859, 1000)\n",
      "  Done copying in 0.685 seconds.\n",
      "Workstations-Data61-L4.obj (73215, 6) (121972, 3)\n",
      "  Done extracting from file in 0.001 seconds.\n",
      "   (73215, 1000) (73215, 1000)\n",
      "  Done copying in 0.699 seconds.\n",
      "Kitchen-Data61-L4.obj (102337, 6) (176558, 3)\n",
      "  Done extracting from file in 0.001 seconds.\n",
      "   (102337, 1000) (102337, 1000)\n",
      "  Done copying in 1.094 seconds.\n",
      "HallWayToKitchen-Data61-L4.obj (26398, 6) (44989, 3)\n",
      "  Done extracting from file in 0.001 seconds.\n",
      "   (26398, 1000) (26398, 1000)\n",
      "  Done copying in 0.300 seconds.\n",
      "StairWell-Data61-L4.obj (77903, 6) (129848, 3)\n",
      "  Done extracting from file in 0.001 seconds.\n",
      "   (77903, 1000) (77903, 1000)\n",
      "  Done copying in 0.803 seconds.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Extracting the NNs we got from the previous step and storing it as a \n",
    "    list that we can access to extract partial spaces.\n",
    "\"\"\"\n",
    "extracted_nns = []\n",
    "\n",
    "for object_name, pointCloud, triangles in new_contiguous_point_collection:\n",
    "    \n",
    "    t0 = time.time()\n",
    "    \n",
    "    print(object_name, np.asarray(pointCloud).shape, np.asarray(triangles).shape)\n",
    "    \n",
    "    extracted_distances = []\n",
    "    extracted_indices = []\n",
    "    \n",
    "    with h5py.File('hd5/{}_{}_nn.h5'.format(object_name,nn_range), 'r') as f:\n",
    "        distances = f['distances']\n",
    "        indices = f['indices']\n",
    "        \n",
    "        print(\"  Done extracting from file in {:.3f} seconds.\".format(time.time()-t0))\n",
    "        \n",
    "        extracted_distances = np.copy(distances)\n",
    "        extracted_indices = np.copy(indices)\n",
    "\n",
    "    print(\"  \",extracted_distances.shape, extracted_indices.shape)\n",
    "    \n",
    "    print(\"  Done copying in {:.3f} seconds.\".format(time.time()-t0))\n",
    "    \n",
    "    extracted_nns.append([\n",
    "        object_name,\n",
    "        extracted_distances,\n",
    "        extracted_indices\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetPartialPointCloudOptimizedNearby(\n",
    "    pointCloud,\n",
    "    triangles,\n",
    "    radius = 1,\n",
    "    vertex = [],\n",
    "    verbose = False,\n",
    "    extract = False,\n",
    "    pointCloud_index = 0,\n",
    "    nearby = False,\n",
    "    nearby_threshold = 1.0\n",
    "):\n",
    "\n",
    "    t0 = time.time()\n",
    "    triangle_indices = np.arange(len(triangles))\n",
    "    \n",
    "    if vertex == []:\n",
    "        get_new_triangle = np.random.choice(triangle_indices)\n",
    "    #rint(\"origin-triangle index\",get_new_triangle,\"(Remember, the triangle indices can be more than the point population.)\")\n",
    "        vertex = triangles[get_new_triangle,1]\n",
    "        if verbose: print(\" Computed origin-vertex\",vertex)\n",
    "    \n",
    "    vertex = np.clip(vertex,0,len(pointCloud))\n",
    "\n",
    "    if extract:\n",
    "        nbrs = NearestNeighbors(n_neighbors=len(pointCloud)-1, algorithm='brute').fit(pointCloud[:,:3])\n",
    "        distances, indices = nbrs.kneighbors(pointCloud[:,:3])\n",
    "    \n",
    "    distances = extracted_nns[pointCloud_index][1]\n",
    "    indices = extracted_nns[pointCloud_index][2]\n",
    "    \n",
    "    if nearby:    \n",
    "        local_indexs =  indices[vertex,np.where(distances[vertex]<(radius*nearby_threshold))[0]]\n",
    "        nearby_vertex = np.random.choice(local_indexs)\n",
    "        vertex = nearby_vertex\n",
    "        \n",
    "    original_vertex = pointCloud[vertex]\n",
    "    if verbose: print(\"   \",original_vertex[:3],vertex,pointCloud[vertex,:3])\n",
    "    # makes sure that we don't get a point beyond the pC size\n",
    "    # 1.b\n",
    "    # list EVERYTHING, then update one-by-one\n",
    "\n",
    "    #t1 = time.time()\n",
    "    #print(\"  GetPartialPointCloud: Done getting nearest neighbors {:.3f} \".format(t1-t0))\n",
    "    \n",
    "    partial_pointcloud = []\n",
    "    partial_triangles = []\n",
    "    \n",
    "    #while len(triangle_indices)>0:\n",
    "    t0 = time.time()\n",
    "    # Get a starting vertex\n",
    "    \n",
    "\n",
    "    # Get the acceptable neighbors of the chosen point\n",
    "    acceptable_point_neighbors = indices[vertex,np.where(distances[vertex]<radius)[0]]\n",
    "    depletable_triangles = np.copy(triangles)\n",
    "    #print(len(acceptable_point_neighbors),\"neighbors\")\n",
    "    acceptable_neighbor_distances = distances[vertex,np.where(distances[vertex]<radius)[0]]\n",
    "    \n",
    "    #t2 = time.time()\n",
    "    #print(\"  GetPartialPointCloud: Done getting acceptable neighbors {:.3f} \".format(t2-t1))\n",
    "\n",
    "    # While distance of points to be addded are less than 1,\n",
    "    # get the indices of the connected items.\n",
    "    prev_length = 0\n",
    "    stopcount = 0\n",
    "\n",
    "    # Get all triangles with the index in the neighbor list\n",
    "    for v in acceptable_point_neighbors:    \n",
    "        included_triangle_indices = np.concatenate((\n",
    "            np.where(depletable_triangles[:,0]==v)[0],\n",
    "            np.where(depletable_triangles[:,1]==v)[0],\n",
    "            np.where(depletable_triangles[:,2]==v)[0]),0)\n",
    "\n",
    "        local_triangles = depletable_triangles[np.unique(included_triangle_indices)]\n",
    "        depletable_triangles = np.delete(depletable_triangles,np.unique(included_triangle_indices),0)\n",
    "\n",
    "        if len(partial_triangles) > 0:\n",
    "            partial_triangles = np.concatenate((partial_triangles,local_triangles),0)\n",
    "            partial_triangles = np.asarray(partial_triangles)\n",
    "        else:\n",
    "            partial_triangles = local_triangles\n",
    "\n",
    "    included_vertices = np.unique(partial_triangles.flatten('C'))\n",
    "    index_list = []\n",
    "    \n",
    "    #t3 = time.time()\n",
    "    #print(\"  GetPartialPointCloud: Done getting vertices {:.3f} \".format(t3 - t2))\n",
    "\n",
    "    for in_vertex in included_vertices:\n",
    "        if in_vertex in acceptable_point_neighbors:\n",
    "            # before adding to the partial lists, check if in the acceptable neighbor list.\n",
    "\n",
    "            partial_pointcloud.append(pointCloud[in_vertex])\n",
    "            #depletable_neighbors = np.delete(depletable_neighbors,np.where(depletable_neighbors==in_vertex),0)\n",
    "            index_list.append(in_vertex)\n",
    "            np.place(partial_triangles,partial_triangles==in_vertex,len(partial_pointcloud)-1)\n",
    "        else:\n",
    "            # if not, remove associated triangles\n",
    "            partial_triangles = np.delete(partial_triangles,np.unique(np.where(partial_triangles==in_vertex)[0]),0)\n",
    "\n",
    "    #t4 = time.time()\n",
    "    #print(\"  GetPartialPointCloud: Done getting triangles {:.3f} \".format(t4 - t3))\n",
    "    \n",
    "    partial_pointcloud = np.asarray(partial_pointcloud)\n",
    "   \n",
    "    return partial_pointcloud, partial_triangles, original_vertex, vertex\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Compute the descriptors from the extracted point cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptors = []\n",
    "\n",
    "for object_name, pointCloud, triangles in new_contiguous_point_collection:\n",
    "            \n",
    "    t0 = time.time()\n",
    "    try:\n",
    "        t_descriptors, t_keypoints, t_d_c = getSpinImageDescriptors(\n",
    "            pointCloud,\n",
    "            down_resolution = 5,\n",
    "            cylindrical_quantization = [4,5]\n",
    "        )\n",
    "        #print(\"Got the true descriptors\",t_descriptors.shape,t_keypoints.shape)\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "        print(\"Error getting the true descriptors of\",object_name,\"with size\",pointCloud.shape)\n",
    "\n",
    "\n",
    "    print(\"Done with\",object_name,\"in\",time.time()-t0,\"seconds.\")\n",
    "    print(\" \",pointCloud.shape,triangles.shape)\n",
    "    print(\" \",t_keypoints.shape,t_descriptors.shape)\n",
    "        \n",
    "    descriptors.append([\n",
    "        object_name,\n",
    "        t_descriptors,\n",
    "        t_keypoints,\n",
    "        t_d_c\n",
    "    ])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment below if you want to overwrite the exisiting descriptors.\n",
    "\n",
    "\"\"\"\n",
    "    with open('descriptors/new_complete_res5_4by5_descriptors.pickle','wb') as f:\n",
    "        pickle.dump(descriptors,f)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2.1 Creating a synthetic set of partial spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Parameters\n",
    "\"\"\"\n",
    "# We used a radius range of 0.25 to 5.0 in increments of 0.25\n",
    "radius_range = radius_range\n",
    "\n",
    "# We used 100 for our investigation\n",
    "samples = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with 9 samples for radius = 0.50 in 9.621 seconds\n",
      "Done with 19 samples for radius = 0.50 in 9.174 seconds\n",
      "Done with 29 samples for radius = 0.50 in 10.064 seconds\n",
      "Done with 39 samples for radius = 0.50 in 12.174 seconds\n",
      "Done with 49 samples for radius = 0.50 in 14.215 seconds\n",
      "Done with 9 samples for radius = 1.00 in 24.711 seconds\n",
      "Done with 19 samples for radius = 1.00 in 23.558 seconds\n",
      "Done with 29 samples for radius = 1.00 in 21.044 seconds\n",
      "Done with 39 samples for radius = 1.00 in 21.727 seconds\n",
      "Done with 49 samples for radius = 1.00 in 19.003 seconds\n",
      "Done with 9 samples for radius = 1.50 in 25.137 seconds\n",
      "Done with 19 samples for radius = 1.50 in 22.276 seconds\n",
      "Done with 29 samples for radius = 1.50 in 24.552 seconds\n",
      "Done with 39 samples for radius = 1.50 in 17.051 seconds\n",
      "Done with 49 samples for radius = 1.50 in 23.674 seconds\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "NOTE!\n",
    "There is a sample set of partial spaces in testing_samples/{}_partial_point_cloud.\n",
    "\n",
    "You may run this to produce a new set with different parameters (as above).\n",
    "If you wish to run it again, make sure you create the NNs from Step 0.1.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "for radius in radius_range:\n",
    "    \n",
    "    partial_point_collection = []\n",
    "    \n",
    "    t0 = time.time()\n",
    "    \n",
    "    for i in np.arange(samples):\n",
    "\n",
    "        random_object = np.random.choice(point_collection_indices)\n",
    "\n",
    "        object_name = new_contiguous_point_collection[random_object][0]\n",
    "        pointCloud = new_contiguous_point_collection[random_object][1]\n",
    "        triangles = new_contiguous_point_collection[random_object][2]\n",
    "\n",
    "        partial_pointcloud, partial_triangles, original_vertex, _v = GetPartialPointCloudOptimizedNearby(\n",
    "            np.asarray(pointCloud),\n",
    "            np.asarray(triangles),\n",
    "            radius,\n",
    "            pointCloud_index=random_object\n",
    "        )\n",
    "        \n",
    "        partial_point_collection.append([\n",
    "            [random_object, object_name, original_vertex],\n",
    "            partial_pointcloud,\n",
    "            partial_triangles            \n",
    "        ])\n",
    "        \n",
    "        with open('testing_samples/{}_partial_point_cloud.pickle'.format(radius), 'wb') as f:\n",
    "            pickle.dump(partial_point_collection,f)\n",
    "            \n",
    "        if i % 10 == (samples-1)%10:\n",
    "            print(\"Done with {} samples for radius = {:.2f} in {:.3f} seconds\".format(i,radius,time.time() - t0))\n",
    "            t0 = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2.2 Creating a synthetic set of successive partial spaces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Parameters\n",
    "\"\"\"\n",
    "# We used a radius range of 0.25 to 5.0 in increments of 0.25.\n",
    "radius_range = radius_range\n",
    "\n",
    "# For our work, we orignally used 50 samples with further 100 successive releases for our investigation.\n",
    "# Below are lower parameters, change as desired.\n",
    "samples = 50\n",
    "releases = 50\n",
    "\n",
    "# For demonstration purposes, we skip testing some successive samples but we still accumulate them.\n",
    "skip = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jaybie/.conda/envs/3d_env/lib/python3.7/site-packages/ipykernel_launcher.py:16: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Done with 5 releases of sample 2 for radius = 0.50 in 116.182 seconds\n",
      "  Done with 21 releases of sample 2 for radius = 0.50 in 8.945 seconds\n",
      "  Done with 37 releases of sample 2 for radius = 0.50 in 6.495 seconds\n",
      "  Done with 5 releases of sample 9 for radius = 0.50 in 245.268 seconds\n",
      "  Done with 21 releases of sample 9 for radius = 0.50 in 34.525 seconds\n",
      "  Done with 37 releases of sample 9 for radius = 0.50 in 29.056 seconds\n",
      "  Done with 5 releases of sample 16 for radius = 0.50 in 438.862 seconds\n",
      "  Done with 21 releases of sample 16 for radius = 0.50 in 26.414 seconds\n",
      "  Done with 37 releases of sample 16 for radius = 0.50 in 31.480 seconds\n",
      "  Done with 5 releases of sample 23 for radius = 0.50 in 373.925 seconds\n",
      "  Done with 21 releases of sample 23 for radius = 0.50 in 29.036 seconds\n",
      "  Done with 37 releases of sample 23 for radius = 0.50 in 29.998 seconds\n",
      "  Done with 5 releases of sample 30 for radius = 0.50 in 378.730 seconds\n",
      "  Done with 21 releases of sample 30 for radius = 0.50 in 4.150 seconds\n",
      "  Done with 37 releases of sample 30 for radius = 0.50 in 8.854 seconds\n",
      "  Done with 5 releases of sample 37 for radius = 0.50 in 419.988 seconds\n",
      "  Done with 21 releases of sample 37 for radius = 0.50 in 25.880 seconds\n",
      "  Done with 37 releases of sample 37 for radius = 0.50 in 30.942 seconds\n",
      "  Done with 5 releases of sample 44 for radius = 0.50 in 215.141 seconds\n",
      "  Done with 21 releases of sample 44 for radius = 0.50 in 20.518 seconds\n",
      "  Done with 37 releases of sample 44 for radius = 0.50 in 19.316 seconds\n",
      " Done with radius = 0.50 in 2663.667 seconds\n",
      "  Done with 5 releases of sample 2 for radius = 1.00 in 202.932 seconds\n",
      "  Done with 21 releases of sample 2 for radius = 1.00 in 39.050 seconds\n",
      "  Done with 37 releases of sample 2 for radius = 1.00 in 43.276 seconds\n",
      "  Done with 5 releases of sample 9 for radius = 1.00 in 753.908 seconds\n",
      "  Done with 21 releases of sample 9 for radius = 1.00 in 59.639 seconds\n",
      "  Done with 37 releases of sample 9 for radius = 1.00 in 59.531 seconds\n",
      "  Done with 5 releases of sample 16 for radius = 1.00 in 840.275 seconds\n",
      "  Done with 21 releases of sample 16 for radius = 1.00 in 59.438 seconds\n",
      "  Done with 37 releases of sample 16 for radius = 1.00 in 57.663 seconds\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "NOTE!\n",
    "There is a sample set of partial spaces in testing_samples/{}_successive_point_cloud.\n",
    "\n",
    "You may run this to produce a new set with different parameters (as above).\n",
    "If you wish to run it again, make sure you create the NNs from Step 0.1.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "for radius in radius_range:\n",
    "        \n",
    "    successive_point_collection = []\n",
    "\n",
    "    t1 = time.time()\n",
    "\n",
    "    for i in np.arange(samples):\n",
    "\n",
    "        random_object = np.random.choice(point_collection_indices)\n",
    "\n",
    "        object_name = new_contiguous_point_collection[random_object][0]\n",
    "        pointCloud = new_contiguous_point_collection[random_object][1]\n",
    "        triangles = new_contiguous_point_collection[random_object][2]\n",
    "\n",
    "        #growing_p_pointcloud = []\n",
    "\n",
    "        growing_point_collection = []\n",
    "\n",
    "        for release in np.arange(releases):\n",
    "\n",
    "            if release == 0:\n",
    "                # For first release, as usual.\n",
    "                partial_pointcloud, partial_triangles, original_vertex, v_i = GetPartialPointCloudOptimizedNearby(\n",
    "                    np.asarray(pointCloud),\n",
    "                    np.asarray(triangles),\n",
    "                    radius,\n",
    "                    pointCloud_index=random_object,\n",
    "                    #verbose = True\n",
    "                )\n",
    "            else:\n",
    "\n",
    "                # For succeeding release, get a nearby point vertex everytime from the previous vertex\n",
    "                partial_pointcloud, partial_triangles, original_vertex, v_i = GetPartialPointCloudOptimizedNearby(\n",
    "                    np.asarray(pointCloud),\n",
    "                    np.asarray(triangles),\n",
    "                    radius = radius,\n",
    "                    vertex = previous_v_i,\n",
    "                    pointCloud_index=random_object,\n",
    "                    nearby = True,\n",
    "                    nearby_threshold=2.0\n",
    "                    #verbose = True\n",
    "                )\n",
    "                \n",
    "                #print(\"  Previous vertex\",previous_vertex[:3],previous_v_i,pointCloud[previous_v_i,:3])\n",
    "                #print(\"  New vertex\",original_vertex[:3],v_i,pointCloud[v_i,:3])                \n",
    "                \n",
    "                if LA.norm(original_vertex[:3] - previous_vertex[:3]) >= 2.0*radius:\n",
    "                    print(\"  Release: {} Warning: The distance of the succeeding vertices is {:.2f}\"\n",
    "                          .format(release,LA.norm(original_vertex[:3] - previous_vertex[:3]))\n",
    "                         )\n",
    "                    \n",
    "            previous_v_i = v_i\n",
    "            previous_vertex = original_vertex\n",
    "\n",
    "            growing_point_collection.append([\n",
    "                [random_object, object_name, original_vertex],\n",
    "                partial_pointcloud,\n",
    "                partial_triangles            \n",
    "            ])\n",
    "\n",
    "            if (release % int(releases/3) == 5) and (i % 7 == 2):\n",
    "                print(\"  Done with {} releases of sample {} for radius = {:.2f} in {:.3f} seconds\"\n",
    "                      .format(release,i,radius,time.time() - t1))\n",
    "                t1 = time.time()\n",
    "\n",
    "        successive_point_collection.append([\n",
    "            [random_object, object_name],\n",
    "            growing_point_collection\n",
    "        ])\n",
    "\n",
    "        with open('testing_samples/{}_successive_point_cloud.pickle'.format(radius), 'wb') as f:\n",
    "            pickle.dump(successive_point_collection,f)\n",
    "\n",
    "    print(\" Done with radius = {:.2f} in {:.3f} seconds\".format(radius,time.time() - t0))\n",
    "    t0 = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-3d_env] *",
   "language": "python",
   "name": "conda-env-.conda-3d_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
